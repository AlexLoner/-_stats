{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import annoy\n",
    "import nltk\n",
    "import re\n",
    "from gensim.models import word2vec\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "data = pd.read_csv('data', sep='@')\n",
    "# df = pd.concat((data[data.Year == 2016], data[data.Year == 2015]), axis=0, ignore_index=True)\n",
    "df = data.copy()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem(text):\n",
    "    return PorterStemmer().stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def convert(abstract):\n",
    "    '''Преобразует текст для w2v'''\n",
    "    text = re.sub(\"[^a-z.!?]\",\" \", abstract)\n",
    "    words = text.lower().split()\n",
    "    words = [lem(w.replace('.', '')) for w in words if not w in STOPWORDS and len(w) > 3]\n",
    "    return ' '.join(words)\n",
    "\n",
    "vec_len = 400\n",
    "df['cl_abst'] = df.Abstract.apply(convert)\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def tok_abstr(abstr, tokenizer):\n",
    "    return [ s.replace('.', '') for s in tokenizer.tokenize(abstr.strip())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences  = [sent.replace('.', '').split() for row in df.cl_abst for sent in tok_abstr(row, tokenizer) if len(sent) > 2]\n",
    "model = word2vec.Word2Vec(sentences, workers=4, size = vec_len, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_single_sentence(snt, words, model, size):\n",
    "    vector = np.zeros(size)\n",
    "    counter = 1\n",
    "    for w in snt:\n",
    "        if w in words:\n",
    "            vector += model[w]# * weight[w][0, 0] # Хранится как матрица \n",
    "            counter += 1\n",
    "    return vector / counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# words = set([i for i in model.wv.index2word if len(i) > 1])\n",
    "words = set(model.wv.index2word)\n",
    "df['new'] = df.cl_abst.apply(lambda x: x.replace('.', '').split())\n",
    "df['vectors'] = df.new.apply(lambda x : avg_single_sentence(x, words, model, vec_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "t = annoy.AnnoyIndex(vec_len)\n",
    "for i, v in enumerate(df.vectors):\n",
    "    t.add_item(i, v)\n",
    "t.build(20)\n",
    "\n",
    "test= convert('black hole')\n",
    "test = test.replace('.', '').split()\n",
    "vec = avg_single_sentence(test, words, model, vec_len)\n",
    "idx, dist = t.get_nns_by_vector(vec, 30, include_distances=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0999140967270853644\r"
     ]
    }
   ],
   "source": [
    "_list = list(df.cl_abst.apply(lambda x : x.replace('.', '')))\n",
    "vect = TfidfVectorizer(ngram_range=(0,1))\n",
    "tfidf = vect.fit_transform(_list)\n",
    "\n",
    "true_weigth = lambda x :tfidf[:, x].todense().max(axis=0) * np.log10(tfidf.shape[0] / tfidf[:, x].count_nonzero())\n",
    "weight = {}\n",
    "idx = 0\n",
    "for w in vect.get_feature_names():\n",
    "    weight[w] = true_weigth(idx)\n",
    "    idx += 1\n",
    "    print(idx / tfidf.shape[1], end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_tfidf(snt, words, model, size, weight):\n",
    "    vector = np.zeros(size)\n",
    "    counter = 1\n",
    "    for w in snt:\n",
    "        if w in words:\n",
    "            vector += model[w] * weight[w][0, 0] # Хранится как матрица \n",
    "            counter += 1\n",
    "    return vector / counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df['vectors_tfidf'] = df.new.apply(lambda x : avg_tfidf(x, words, model, vec_len, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "t = annoy.AnnoyIndex(vec_len)\n",
    "for i, v in enumerate(df.vectors_tfidf):\n",
    "    t.add_item(i, v)\n",
    "t.build(20)\n",
    "\n",
    "test= convert('black hole')\n",
    "test = test.replace('.', '').split()\n",
    "vec = avg_tfidf(test, words, model, vec_len, weight)\n",
    "idx_tf, dist_tf = t.get_nns_by_vector(vec, 30, include_distances=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://elibrary.ru/item.asp?id=25163288'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Link[858]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(results, s):\n",
    "    print('{} model'.format(s))\n",
    "    print('indexes: {}\\ndistance:{}\\n'.format(results[0], np.round(results[1], 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11641"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single model\n",
      "indexes: [7193, 7022, 1687, 2026, 455, 6174, 2888, 1826, 5041, 2762, 4811, 5498, 126, 0, 3780, 3596, 1793, 3278, 2571, 330, 1532, 264, 6888, 6, 3768, 1228, 1112, 2108, 1, 4652]\n",
      "distance:[0.7887 0.7967 0.8117 0.8155 0.823  0.8252 0.8698 0.881  0.8874 0.8929\n",
      " 0.9116 0.9122 0.9203 0.9321 0.9434 0.9539 0.9607 0.9625 0.9634 0.9654\n",
      " 0.9957 0.9988 1.0076 1.0125 1.0131 1.0255 1.031  1.0328 1.0377 1.0385]\n",
      "\n",
      "TF-IDF model\n",
      "indexes: [7022, 6174, 455, 2026, 1687, 509, 2888, 1826, 2050, 3596, 4811, 126, 2571, 3780, 2762, 5041, 1526, 0, 1793, 330, 4652, 2108, 5498, 3768, 4517, 6, 8293, 1532, 1228, 858]\n",
      "distance:[0.6533 0.7214 0.7318 0.7531 0.7726 0.7746 0.8104 0.8168 0.8236 0.8324\n",
      " 0.8374 0.8406 0.8515 0.8566 0.8675 0.8687 0.8703 0.875  0.882  0.8822\n",
      " 0.8928 0.9077 0.9084 0.9193 0.9218 0.9414 0.9482 0.9491 0.9513 0.9541]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pprint([idx, dist], 'single')\n",
    "pprint([idx_tf, dist_tf], 'TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF model\n",
      "indexes: [7022, 6174, 455, 2026, 1687, 509, 1826, 126, 2571, 3780]\n",
      "distance:[0.6533 0.7214 0.7318 0.7531 0.7726 0.7746 0.8168 0.8406 0.8515 0.8566]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pprint([idx_tf, dist_tf], 'TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(weight, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
