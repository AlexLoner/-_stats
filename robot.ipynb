{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from user_agent import generate_user_agent\n",
    "import selenium\n",
    "from selenium.webdriver import Firefox, Chrome, Remote\n",
    "from selenium.webdriver.common.proxy import *\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstract(name, page):\n",
    "    '''\n",
    "    Функция возвращает абстакт для журнала name\n",
    "    '''\n",
    "    def nature(page):\n",
    "        return page.findAll('p')[5].text\n",
    "    def jetp(page):\n",
    "        return page.findAll('p')[3].text\n",
    "    func = {\n",
    "        'NATURE_PHYSICS' : nature,\n",
    "        'JETP' : jetp,\n",
    "        'JETP_LETTERS' : jetp\n",
    "    }\n",
    "    return func[name](page)\n",
    "\n",
    "def get_articles_adress(page):\n",
    "    '''\n",
    "    Возвращает список адресов всех статей\n",
    "    '''\n",
    "    articles_adress = []\n",
    "    id_str = '/item.asp?id='\n",
    "    for i in page.findAll('a'):\n",
    "        try:\n",
    "            if id_str in i['href']:\n",
    "                num_id = i['href']\n",
    "                articles_adress.append('https://elibrary.ru' + num_id)\n",
    "        except:\n",
    "            continue\n",
    "    return articles_adress\n",
    "\n",
    "def find_data(browser, name, adress):\n",
    "    '''\n",
    "    Возвращает list из [названия статьи, авторы, число цитирований, абстракт]\n",
    "    '''\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    sleep(np.random.rand() * 2)\n",
    "    data = []\n",
    "    # Добавление Автора и Названия статьи\n",
    "    for i in soup.findAll('input'):\n",
    "        try:    \n",
    "            if not i['value'][0].isdigit() and (i['value'][0] == i['value'][0].upper()):\n",
    "                data.append(i['value'])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(data) < 2:\n",
    "        data.insert(0, 'Incognito')\n",
    "    # Добавление количества цитирований\n",
    "    try:\n",
    "        quot = soup.findAll('a', attrs= {'href':['cit_items.asp?id=' + adress.split('=')[-1]]})[0].text\n",
    "    except:\n",
    "        quot = '0'\n",
    "    data.append(quot)\n",
    "    # Добавление абстракта\n",
    "    abstract = soup.find('p').text\n",
    "    key = 'https://doi'\n",
    "    if abstract:\n",
    "        data.append(abstract)\n",
    "    else:\n",
    "        for i in soup.findAll('a'):\n",
    "            try:\n",
    "                if key in i['href']:\n",
    "                    adress_r = i['href']\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        browser.get(adress_r)\n",
    "        ppp = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        abstract_journal = get_abstract(name, ppp)\n",
    "        sleep(4 * np.random.random())\n",
    "        browser.back()\n",
    "        data.append(abstract_journal) #если нет абстракта берет его с сайта журнала\n",
    "    return data\n",
    "\n",
    "def run_to_actual_page(page_num,browser,xp_main):\n",
    "    for i in range(page_num):\n",
    "        sleep(7)\n",
    "        search_form = browser.find_element_by_xpath(xp_main)\n",
    "        search_form.click()                                                                  # Жмякаем на кнопку\n",
    "\n",
    "def data_frame(name):\n",
    "    try:\n",
    "        return pd.read_csv(name,sep=\"@\")\n",
    "    except:\n",
    "        return pd.DataFrame(columns=['Authors', 'Title', 'Qoutes', 'Abstract', 'Link'])\n",
    "    \n",
    "def parse_journal(name, browser, page_num):\n",
    "    print(name)\n",
    "    print()\n",
    "    columns = ['Authors', 'Title', 'Qoutes', 'Abstract', 'Link']\n",
    "    articles = data_frame(name)\n",
    "    xp_main = '//*[@id=\"pages\"]/table/tbody/tr/td[13]/a'\n",
    "    run_to_actual_page(page_num, browser, xp_main)\n",
    "    pp = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    runs = int(pp.find('font',attrs={'color':['#ff0000']}).text) // 20\n",
    "    print('runs:', runs)\n",
    "    sleep(5)\n",
    "    for loop in range(page_num, runs):\n",
    "        \n",
    "        search_form = browser.find_element_by_xpath(xp_main)\n",
    "        page = BeautifulSoup(browser.page_source, \"html.parser\")                             \n",
    "        new_id = get_articles_adress(page)                                                   \n",
    "        if loop % 2:\n",
    "            sleep(5 + 15 * np.random.random())\n",
    "        for i in np.random.choice(range(len(new_id)), 15, replace=False):\n",
    "            if i % 3 == 0:\n",
    "                sleep(3 * np.random.random() + 1.5 * np.random.random())\n",
    "            browser.get(new_id[i])                         \n",
    "            d = find_data(browser, name, new_id[i])\n",
    "            d.append(new_id[i])                                                                    \n",
    "            print(\"___{0}___{1}___\".format(loop, i), end='\\r')\n",
    "            sleep(2 + 5 * np.random.random())\n",
    "            browser.back()\n",
    "            articles = articles.append(pd.Series(d, index=columns), ignore_index=True)\n",
    "            articles.to_csv(name, sep=\"@\")\n",
    "        search_form = browser.find_element_by_xpath(xp_main)\n",
    "        search_form.click()                                                                  \n",
    "        sleep(5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "NATURE_PHYSICS\n",
      "\n",
      "runs: 28\n",
      "___0___6___\r"
     ]
    }
   ],
   "source": [
    "browser = Chrome(executable_path=\"./chromedriver\")   \n",
    "browser.get(k)\n",
    "journals = {\n",
    "    'NATURE_PHYSICS'    : 'https://elibrary.ru/title_items.asp?id=25368',\n",
    "    'JETP' : 'https://elibrary.ru/title_items.asp?id=7467',\n",
    "    'JETP_LETTERS' : 'https://elibrary.ru/title_items.asp?id=7468',\n",
    "    'PHYSICAL_REVIEW_B' : \"https://elibrary.ru/title_items.asp?id=21814\",\n",
    "    'PHYSICAL_REVIEW_C' : \"https://elibrary.ru/title_items.asp?id=21815\",\n",
    "    'PHYSICAL_REVIEW_LETTERS' : \"https://elibrary.ru/title_items.asp?id=21820\",\n",
    "}\n",
    "for i, k in journals.items():\n",
    "    browser.get(k)\n",
    "    r = input()\n",
    "    parse_journal(i, browser, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Chrome(executable_path=\"./chromedriver\")   \n",
    "browser.get('https://elibrary.ru/item.asp?id=26841171')\n",
    "p = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "# get_abstract('JETP_LETTERS', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    quot = p.findAll('a', attrs= {'href':['cit_items.asp?id=' + adress.split('=')[-1]]})[0].text\n",
    "except:\n",
    "    quot = '0'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
