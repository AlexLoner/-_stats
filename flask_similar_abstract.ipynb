{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "/home/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "127.0.0.1 - - [26/Nov/2018 02:57:57] \"GET /get_similar_articles/Dark%20MAtter%20Field%20Gravity%20repulsion HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num [7687, 2230, 2439, 2390, 5926]\n",
      "[1.0177284479141235, 1.1211655139923096, 1.1211655139923096, 1.1211655139923096, 1.1557844877243042]\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import annoy\n",
    "import nltk\n",
    "import re\n",
    "from gensim.models import word2vec\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "app = Flask(__name__)\n",
    "def convert(abstract):\n",
    "    '''Преобразует текст для w2v'''\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = re.sub(\"[^a-zA-Z.!?]\",\" \", abstract)\n",
    "    words = text.lower().split()\n",
    "    words = [w for w in words if not w in stops if len(w) >= 2]\n",
    "    words = [PorterStemmer().stem(w) for w in words ] # lemmitization\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tok_abstr(abstr, tokenizer):\n",
    "    '''Разделяет абстракт на отдельные предложения'''\n",
    "    return [s.replace('.', '') for s in tokenizer.tokenize(abstr.strip())]\n",
    "\n",
    "def avg_single_sentence(snt, words, model, size):\n",
    "    '''Возвращает усредненный вектор для абстракта'''\n",
    "    vector = np.zeros(size)\n",
    "    counter = 0\n",
    "    for w in snt:\n",
    "        if w in words:\n",
    "            vector += model[w]\n",
    "            counter += 1\n",
    "    return vector / counter\n",
    "\n",
    "def init_sim_arctiles(filename):\n",
    "    ex = pd.read_csv(filename, sep='@')\n",
    "    ex = ex.dropna()\n",
    "    vect_len = 400\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    ex['cl_abst'] = ex.Abstract.apply(convert)  # Подготовка данных для w2v\n",
    "    ex['new'] = ex.cl_abst.apply(lambda x : tok_abstr(x, tokenizer)) # будет нужно для создания list of lists для model\n",
    "    sentences  = [sent.replace('.', '').split() for row in ex.cl_abst for sent in tok_abstr(row, tokenizer) if len(sent) >= 1]\n",
    "    model = word2vec.Word2Vec(sentences, size=vect_len, window=50, workers=4)\n",
    "    words = set(model.wv.index2word)\n",
    "    vectors = [avg_single_sentence(x, words, model, vect_len) for x in ex.new]\n",
    "    return (vect_len, words, model, vectors, ex)\n",
    "\n",
    "def add_art(df, num):\n",
    "    art = {}\n",
    "    for i in num:\n",
    "        art[df.Title.iloc[i]] = df.Link.iloc[i]\n",
    "    return art\n",
    "\n",
    "@app.route(\"/get_similar_articles/<string>\", methods=['GET', 'POST'])\n",
    "def return_art(string):\n",
    "    vect_len, words, model, vectors, df = init_sim_arctiles('data')\n",
    "    obj = annoy.AnnoyIndex(vect_len)\n",
    "    for i, v in enumerate(vectors):\n",
    "        obj.add_item(i, v)\n",
    "    obj.build(20)\n",
    "    try:\n",
    "        w = convert(string)\n",
    "        vec = avg_single_sentence(w.split(), words, model, vect_len)\n",
    "        num, d = obj.get_nns_by_vector(vec, 5, include_distances=True)\n",
    "        print('num', num)\n",
    "        print(d)\n",
    "        guess = add_art(df, num)\n",
    "    except: \n",
    "        guess = \"Nothing was found\"\n",
    "    return jsonify(guess)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return 'Hi there. Try ~/get_sudgest/<string> or /get_similar_articles/<string>'\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
