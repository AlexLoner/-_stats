{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Building a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [28/Nov/2018 10:15:16] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Nov/2018 10:15:17] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making annoy tree...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Nov/2018 10:15:28] \"GET /get_similar_articles/dark%20matter HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying return guesses...\n"
     ]
    }
   ],
   "source": [
    "# %%writefile flask_similar_abstract.py\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import annoy\n",
    "import nltk\n",
    "import re\n",
    "from gensim.models import word2vec\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import pickle\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def lem(text):\n",
    "    return PorterStemmer().stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def tok_abstr(abstr, tokenizer):\n",
    "    return [ s.replace('.', '') for s in tokenizer.tokenize(abstr.strip())]\n",
    "\n",
    "def convert(abstract):\n",
    "    '''Преобразует текст для w2v'''\n",
    "    text = re.sub(\"[^a-z.!?]\",\" \", abstract)\n",
    "    words = text.lower().split()\n",
    "    words = [lem(w.replace('.', '')) for w in words if not w in STOPWORDS and len(w) > 3]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def avg_tfidf(snt, words, model, size, weight):\n",
    "    vector = np.zeros(size)\n",
    "    counter = 1\n",
    "    for w in snt:\n",
    "        if w in words:\n",
    "            vector += model[w] * weight[w][0, 0] \n",
    "            counter += 1\n",
    "    return vector / counter\n",
    "\n",
    "def init_sim_arctiles(filename):\n",
    "    df = pd.read_csv('data', sep='@')\n",
    "    df = df.dropna()\n",
    "    vec_len = 400\n",
    "    print('Preparing data...')\n",
    "    df['cl_abst'] = df.Abstract.apply(convert)\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences  = [sent.replace('.', '').split() for row in df.cl_abst for sent in tok_abstr(row, tokenizer) if len(sent) > 1]\n",
    "    model = word2vec.Word2Vec(sentences, workers=4, size = vec_len, window = 5)\n",
    "    words = set(model.wv.index2word)\n",
    "    df['new'] = df.cl_abst.apply(lambda x: x.replace('.', '').split())\n",
    "    print('Building a model...')\n",
    "    _list = list(df.cl_abst.apply(lambda x : x.replace('.', '')))\n",
    "    vect = TfidfVectorizer(ngram_range=(0,1))\n",
    "    tfidf = vect.fit_transform(_list)\n",
    "    weight = load_obj('weight')\n",
    "#     true_weigth = lambda x :tfidf[:, x].todense().max(axis=0) * np.log10(tfidf.shape[0] / tfidf[:, x].count_nonzero())\n",
    "#     weight = {}\n",
    "#     idx = 0\n",
    "#     for w in vect.get_feature_names():\n",
    "#         weight[w] = true_weigth(idx)\n",
    "#         idx += 1\n",
    "#         print(idx / tfidf.shape[1], end='\\r')\n",
    "    df['vectors_tfidf'] = df.new.apply(lambda x : avg_tfidf(x, words, model, vec_len, weight))\n",
    "    return (vec_len, words, model, df, weight)\n",
    "\n",
    "def add_art(df, num):\n",
    "    art = {}\n",
    "    for i in num:\n",
    "        art[df.Title.iloc[i]] = df.Link.iloc[i]\n",
    "    return art\n",
    "\n",
    "vec_len, words, model, df, weight = init_sim_arctiles('data')\n",
    "@app.route(\"/get_similar_articles/<string>\", methods=['GET', 'POST'])\n",
    "def return_art(string):\n",
    "#     vec_len, words, model, df, weight = init_sim_arctiles('data')\n",
    "    print('Making annoy tree...')\n",
    "    obj = annoy.AnnoyIndex(vec_len)\n",
    "    for i, v in enumerate(df.vectors_tfidf):\n",
    "        obj.add_item(i, v)\n",
    "    obj.build(20)\n",
    "    try:\n",
    "        print('Trying return guesses...')\n",
    "        w = convert(string)\n",
    "        w = w.replace('.', '').split()\n",
    "        vec = avg_tfidf(w, words, model, vec_len, weight)\n",
    "        num = obj.get_nns_by_vector(vec, 5)\n",
    "        guess = add_art(df, num)\n",
    "    except: \n",
    "        guess = \"Nothing was found\"\n",
    "    return jsonify(guess)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return 'Hi there. Try ~ /get_similar_articles/<string>'\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
